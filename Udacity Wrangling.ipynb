{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenStreet Data Wrangling with Python and SQL\n",
    "# This one single file will include all queries, wrangling and details related to every part of the project, to bring more consistency to our project as we try equally hard with our Map. \n",
    "\n",
    "### Project Summary\n",
    "### Map area: Cleveland, Ohio, United States\n",
    "\n",
    "This map is of a place that is home to my favourite NBA Team, the Cavaliers. This reveals some inconsistencies in data and contributes to its improvement on OpenStreetMap.org.\n",
    "\n",
    "Data auditing and processing:\n",
    "\n",
    "## After downloading and auditing the Ohio area dataset, I noticed some inconsistencies in representing data:\n",
    "1) Street names were inconsistent.\n",
    "Abbreviations St -> Street\n",
    "Dots at the end St. -> Street\n",
    "Lowercase street -> Street\n",
    "\n",
    "2) Zip codes had different formats.\n",
    "5-4 digit formats. \n",
    "State abbreviations with zip code OH44118 -> 44118\n",
    "\n",
    "Changed all street names to consistent forms and zip codes to correct formats.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the required libraries/packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "import re\n",
    "import codecs\n",
    "import json\n",
    "import pprint\n",
    "import maincode #This is the parent file from which we derive our REGEX.\n",
    "OSMFILE = \"cleveland_ohio.osm\"\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bounds': 1, 'tag': 1162747, 'node': 1857212, 'nd': 2164787, 'way': 197433, 'member': 26049, 'relation': 3839, 'osm': 1}\n"
     ]
    }
   ],
   "source": [
    "tags = dict() #To check how many types of tags we have available in our data.\n",
    "for _,j in ET.iterparse(OSMFILE):\n",
    "    if j.tag in tags:\n",
    "        tags[j.tag]+=1\n",
    "    else:\n",
    "        tags[j.tag]=1\n",
    "        \n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "street_type_re = maincode.street_type_re\n",
    "\n",
    "expected = maincode.expected\n",
    "\n",
    "# After auditing street names these new mappings were created to map variations to expected\n",
    "street_name_mapping = {\n",
    "    'St': 'Street',\n",
    "    'Ave': 'Avenue',\n",
    "    'Ave.': 'Avenue',\n",
    "    'Blvd': 'Boulevard',\n",
    "    'Blvd.': 'Boulevard',\n",
    "    'Dr': 'Drive',\n",
    "    'Dr.': 'Drive',\n",
    "    'Ln': 'Lane',\n",
    "    'Pkwy': 'Parkway',\n",
    "    'Rd': 'Road',\n",
    "    'Rd.': 'Road',\n",
    "    'St': 'Street',\n",
    "    'St.': 'Street',\n",
    "    'Street.': 'Street',\n",
    "    'ave': 'Avenue'\n",
    "}\n",
    "\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "\n",
    "# is this element in the format of expected street key\n",
    "def is_street_name(elem):\n",
    "    return elem.attrib['k'] == \"addr:street\"\n",
    "\n",
    "\n",
    "# audit method to extract the street types\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types\n",
    "\n",
    "if False:\n",
    "    pprint.pprint(dict(audit(OSMFILE)))\n",
    "\n",
    "\n",
    "#Update names to expected values\n",
    "def update_name(name, mapping):\n",
    "    better_name = name\n",
    "    m = street_type_re.search(name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type in mapping:\n",
    "            better_name = re.sub(street_type, mapping[street_type], name)\n",
    "    return better_name\n",
    "\n",
    "\n",
    "#REGEX to test keys\n",
    "lower = maincode.lower\n",
    "lower_colon = maincode.lower_colon\n",
    "problemchars = maincode.problemchars\n",
    "\n",
    "# The valid Cleveland,Ohio zip codes should fall in the range of 43001 and 45999\n",
    "def update_zip_code(zip):\n",
    "    matched = zip_code_re.match(zip)\n",
    "    if matched:\n",
    "        zip_code = int(matched.group(1))\n",
    "        if zip_code >= 43001 and zip_code <= 45999:\n",
    "            return zip_code\n",
    "    return None\n",
    "\n",
    "# Helper function to see if the value is a street\n",
    "def is_street_tag(key_array):\n",
    "    if len(key_array) > 1 and key_array[0] == 'addr' and key_array[1] == 'street':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# Assumes the tag is a street tag with only one : that splits it\n",
    "def is_street_tag_only(street_tag):\n",
    "    return len(street_tag) == 2\n",
    "\n",
    "\n",
    "def is_postal_tag(key_array):\n",
    "    if len(key_array) > 1 and key_array[1] == 'postcode':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "tiger_left = re.compile(r'^tiger:zip_left$')\n",
    "tiger_right = re.compile(r'^tiger:zip_right$')\n",
    "\n",
    "\n",
    "# for auditing tiger zips used\n",
    "zips_from_tiger = set()\n",
    "\n",
    "# search an element for tiger zip tags to add to our nodes\n",
    "# if it has a postal code, use that...else check tiger tags\n",
    "# if it has either a left or right only return that cleaned zip code\n",
    "# if it has a left and right, check they are equal and return\n",
    "# if not equal, or not a valid zip or does not contain tag, return none\n",
    "def get_best_matched_zip(element):\n",
    "    left_zip = None\n",
    "    right_zip = None\n",
    "    postal_zip = None\n",
    "    for tag in element.iter(\"tag\"):\n",
    "        if 'k' in tag.attrib:\n",
    "            key_array = tag.attrib['k'].split(\":\")\n",
    "            if is_postal_tag(key_array):\n",
    "                postal_zip = update_zip_code(tag.attrib['v'])\n",
    "                break\n",
    "            matched = tiger_left.search(tag.attrib['k'])\n",
    "            if matched:\n",
    "                left_zip = update_zip_code(tag.attrib['v'])\n",
    "                continue\n",
    "            matched = tiger_right.search(tag.attrib['k'])\n",
    "            if matched:\n",
    "                right_zip = update_zip_code(tag.attrib['v'])\n",
    "                continue\n",
    "    if postal_zip:\n",
    "        return postal_zip\n",
    "    if left_zip and right_zip and left_zip == right_zip:\n",
    "        zips_from_tiger.add(left_zip)\n",
    "        return left_zip\n",
    "    if left_zip:\n",
    "        zips_from_tiger.add(left_zip)\n",
    "        return left_zip\n",
    "    if right_zip:\n",
    "        zips_from_tiger.add(right_zip)\n",
    "        return right_zip\n",
    "    return None\n",
    "\n",
    "#Filter list so we do not overwrite our primary schema keys\n",
    "keys_to_ignore = [\"type\", \"id\", \"visible\", \"created\", \"address\", \"addr:postcode\"]\n",
    "tiger_tag = maincode.tiger_tag\n",
    "tag_keys = defaultdict()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zip_code_re = re.compile(r'(\\d{5})',re.UNICODE) #Rewrite the zip code REGEX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following schema code is borrowed from the schema.py Udacity code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "SCHEMA = {\n",
    "    'node': {\n",
    "        'type': 'dict',\n",
    "        'schema': {\n",
    "            'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'lat': {'required': True, 'type': 'float', 'coerce': float},\n",
    "            'lon': {'required': True, 'type': 'float', 'coerce': float},\n",
    "            'user': {'required': True, 'type': 'string'},\n",
    "            'uid': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'version': {'required': True, 'type': 'string'},\n",
    "            'changeset': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'timestamp': {'required': True, 'type': 'string'}\n",
    "        }\n",
    "    },\n",
    "    'node_tags': {\n",
    "        'type': 'list',\n",
    "        'schema': {\n",
    "            'type': 'dict',\n",
    "            'schema': {\n",
    "                'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'key': {'required': True, 'type': 'string'},\n",
    "                'value': {'required': True, 'type': 'string'},\n",
    "                'type': {'required': True, 'type': 'string'}\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'way': {\n",
    "        'type': 'dict',\n",
    "        'schema': {\n",
    "            'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'user': {'required': True, 'type': 'string'},\n",
    "            'uid': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'version': {'required': True, 'type': 'string'},\n",
    "            'changeset': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'timestamp': {'required': True, 'type': 'string'}\n",
    "        }\n",
    "    },\n",
    "    'way_nodes': {\n",
    "        'type': 'list',\n",
    "        'schema': {\n",
    "            'type': 'dict',\n",
    "            'schema': {\n",
    "                'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'node_id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'position': {'required': True, 'type': 'integer', 'coerce': int}\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'way_tags': {\n",
    "        'type': 'list',\n",
    "        'schema': {\n",
    "            'type': 'dict',\n",
    "            'schema': {\n",
    "                'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'key': {'required': True, 'type': 'string'},\n",
    "                'value': {'required': True, 'type': 'string'},\n",
    "                'type': {'required': True, 'type': 'string'}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The next function defines our \"SHAPER\" that helps frame our data into the required format to be then converted to .csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shape_element(element):\n",
    "    node = {}\n",
    "    if element.tag == \"node\" or element.tag == \"way\":\n",
    "        if element.tag == \"node\":\n",
    "            node['type'] = \"node\"\n",
    "        else:\n",
    "            node['type'] = \"way\"\n",
    "\n",
    "        node['id'] = element.attrib['id']\n",
    "\n",
    "        if 'visible' in element.attrib:\n",
    "            node['visible'] = element.attrib['visible']\n",
    "\n",
    "        if 'lat' in element.attrib and 'lon' in element.attrib:\n",
    "            node['pos'] = []\n",
    "            node['pos'].append(float(element.attrib['lat']))\n",
    "            node['pos'].append(float(element.attrib['lon']))\n",
    "\n",
    "        for c in CREATED:\n",
    "            if c in element.attrib:\n",
    "                if 'created' not in node:\n",
    "                    node['created'] = {}\n",
    "                node['created'][c] = element.attrib[c]\n",
    "\n",
    "        if element.tag == \"way\":\n",
    "            zip_code = get_best_matched_zip(element)\n",
    "            # added postal codes to the address when present\n",
    "            # clean zip and ignore non standard zips before attempting to add\n",
    "            if zip_code:\n",
    "                if 'address' not in node:\n",
    "                    node['address'] = {}\n",
    "                node['address']['postcode'] = zip_code\n",
    "\n",
    "            for tag in element.iter(\"tag\"):\n",
    "                matched = problemchars.search(tag.attrib['k'])\n",
    "                if matched is None:\n",
    "                    key_array = tag.attrib['k'].split(':')\n",
    "                    value = tag.attrib['v']\n",
    "                    \n",
    "                    if is_street_tag(key_array):\n",
    "                        if 'address' not in node:\n",
    "                            node['address'] = {}\n",
    "                        # only add the tag if it is just the street, no extra : delimiter\n",
    "                        if is_street_tag_only(key_array):\n",
    "                            node['address']['street'] = update_name(value, street_name_mapping)\n",
    "\n",
    "                    if 'k' in tag.attrib and tag.attrib['k'] not in keys_to_ignore:\n",
    "                        matched = tiger_tag.search(tag.attrib['k'])\n",
    "                        if not matched:\n",
    "                            tag_keys[tag.attrib['k']] = 1\n",
    "                            node[tag.attrib['k']] = value\n",
    "\n",
    "            for tag in element.iter(\"nd\"):\n",
    "                if 'node_refs' not in node:\n",
    "                    node['node_refs'] = []\n",
    "                if 'ref' in tag.attrib:\n",
    "                    node['node_refs'].append(tag.attrib['ref'])\n",
    "\n",
    "        return node\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i,j in ET.iterparse(OSMFILE): #Calling the main shaper function on each element.\n",
    "    j = shape_element(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The next code was referenced from the Quiz number 11 of the final lesson.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "import csv\n",
    "import codecs\n",
    "import pprint\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "\n",
    "import cerberus\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  \n",
    "\n",
    "    if element.tag == 'node':\n",
    "        for attrib in element.attrib:\n",
    "            if attrib in NODE_FIELDS:\n",
    "                node_attribs[attrib] = element.attrib[attrib]\n",
    "        \n",
    "        for child in element:\n",
    "            node_tag = {}\n",
    "            if LOWER_COLON.match(child.attrib['k']):\n",
    "                node_tag['type'] = child.attrib['k'].split(':',1)[0]\n",
    "                node_tag['key'] = child.attrib['k'].split(':',1)[1]\n",
    "                node_tag['id'] = element.attrib['id']\n",
    "                node_tag['value'] = child.attrib['v']\n",
    "                tags.append(node_tag)\n",
    "            elif PROBLEMCHARS.match(child.attrib['k']):\n",
    "                continue\n",
    "            else:\n",
    "                node_tag['type'] = 'regular'\n",
    "                node_tag['key'] = child.attrib['k']\n",
    "                node_tag['id'] = element.attrib['id']\n",
    "                node_tag['value'] = child.attrib['v']\n",
    "                tags.append(node_tag)\n",
    "        \n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "        \n",
    "    elif element.tag == 'way':\n",
    "        for attrib in element.attrib:\n",
    "            if attrib in WAY_FIELDS:\n",
    "                way_attribs[attrib] = element.attrib[attrib]\n",
    "        \n",
    "        position = 0\n",
    "        for child in element:\n",
    "            way_tag = {}\n",
    "            way_node = {}\n",
    "            \n",
    "            if child.tag == 'tag':\n",
    "                if LOWER_COLON.match(child.attrib['k']):\n",
    "                    way_tag['type'] = child.attrib['k'].split(':',1)[0]\n",
    "                    way_tag['key'] = child.attrib['k'].split(':',1)[1]\n",
    "                    way_tag['id'] = element.attrib['id']\n",
    "                    way_tag['value'] = child.attrib['v']\n",
    "                    tags.append(way_tag)\n",
    "                elif PROBLEMCHARS.match(child.attrib['k']):\n",
    "                    continue\n",
    "                else:\n",
    "                    way_tag['type'] = 'regular'\n",
    "                    way_tag['key'] = child.attrib['k']\n",
    "                    way_tag['id'] = element.attrib['id']\n",
    "                    way_tag['value'] = child.attrib['v']\n",
    "                    tags.append(way_tag)\n",
    "                    \n",
    "            elif child.tag == 'nd':\n",
    "                way_node['id'] = element.attrib['id']\n",
    "                way_node['node_id'] = child.attrib['ref']\n",
    "                way_node['position'] = position\n",
    "                position += 1\n",
    "                way_nodes.append(way_node)\n",
    "        \n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.items())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_string = pprint.pformat(errors)\n",
    "        \n",
    "        raise Exception(message_string.format(field, error_string))\n",
    "\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: v for k, v in row.items()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "def process_map(file_in, validate):\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "         codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "         codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "         codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "         codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Note: Validation is ~ 10X slower. For the project consider using a small\n",
    "    # sample of the map when validating.\n",
    "    process_map(OSMFILE, validate=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After importing the data from .csv files to the in-memory database using \".mode csv\", we will save it to a database in our storage for better access and backup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some basic details before we begin the querying using SQLite3 drivers for Python: \n",
    "\n",
    "## 1. Size of the uncompressed .OSM file : 415.6Mb\n",
    "## 2. Size of the SQL database : 297.2Mb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The number of nodes and ways: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1857212 nodes in our database.\n",
      "There are 197433 ways in our database.\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect(\"shantanu2.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "#Executing and printing the queries. \n",
    "cursor.execute(\"select count(id) from nodes;\")\n",
    "print('There are {} nodes in our database.'.format(cursor.fetchall()[0][0]))\n",
    "\n",
    "cursor.execute(\"select count(id) from ways;\")\n",
    "print('There are {} ways in our database.'.format(cursor.fetchall()[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The number of unique users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1360 unique users in our database.\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"select count(distinct(uid)) from (select uid from nodes union select uid from ways);\")\n",
    "print('There are {} unique users in our database.'.format(cursor.fetchall()[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The number of nodes in the biggest way and the biggest way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1754 nodes in the biggest way of our database. The Way id is 37881132.\n",
      "This way is:\n",
      "[('37881132', 'name', 'West Branch Rocky River', 'regular'),\n",
      " ('37881132', 'source', 'Yahoo;bing', 'regular'),\n",
      " ('37881132', 'waterway', 'river', 'regular')]\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"select id, count(*) as nodes_count from ways_nodes group by id order by nodes_count desc limit 1;\")\n",
    "way_id, count = cursor.fetchall()[0]\n",
    "print(\"There are {} nodes in the biggest way of our database. The Way id is {}.\".format(count, way_id))\n",
    "\n",
    "cursor.execute(\"select * from ways_tags where id = {};\".format(way_id))\n",
    "print('This way is:')\n",
    "pprint.pprint(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The number of bridges in Ohio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 50 bridges in Ohio.\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"select count(key) from ways_tags where key = 'bridge' and value != 'yes' group by key;\")\n",
    "print(\"There are {} bridges in Ohio.\".format(cursor.fetchall()[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The top 7 postcodes in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum count of top 7 postcodes in this dataset are:\n",
      "[('44118', 450),\n",
      " ('44124', 209),\n",
      " ('44113', 142),\n",
      " ('44115', 103),\n",
      " ('44106', 94),\n",
      " ('44123', 85),\n",
      " ('44114', 75)]\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"select value, count(*) as count from nodes_tags where key = 'postcode' group by value order by count desc limit 7;\")\n",
    "print(\"The maximum count of top 7 postcodes in this dataset are:\")\n",
    "pprint.pprint(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONCLUSION : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# During my analysis, I've seen large amount of data that has not been correctly formatted and cleaned. But I successfully iterparsed this data and corrected streets, zip codes etc. The bigger issue is that the .osm data has a lot of inconsistencies. Sometimes it's very difficult to find these inconsistencies.\n",
    "\n",
    "## There is a lot of work to be done to complete this map.\n",
    "\n",
    "## OpenStreetMap data is not perfect as any human modified project isn't. It'll take a lot of time to find and clean all human-made errors. But we've made our first step. We modified street names and made them more consistent and uniform. Then we transformed XML to CSV format and imported it into an SQL database. And finally we answered some interesting questions using SQL queries.\n",
    "\n",
    "## Additional ideas:\n",
    "\n",
    "## In my opinion there's two ways to improve OpenStreenData project.\n",
    "First of all it's extremely important to attract more people to improving maps. My suggestion would be the use of gamification. It's reasonable to establish ranking systems like Kaggle or badge systems like Khan Academy.\n",
    "\n",
    "## Benefits of this:\n",
    "### Increase in productivity\n",
    "### Help to retain high performers by involving them into moderation\n",
    "## Anticipated problems:\n",
    "### Has very small effect on results\n",
    "### Creates competition that can be counterproductive\n",
    "\n",
    "\n",
    "## Second is to use different sources to cross-validate inconsistencies and empty spots on OS maps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
